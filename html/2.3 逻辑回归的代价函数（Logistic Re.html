<!DOCTYPE html>
<html>
<head>
<title>2.3 逻辑回归的代价函数（Logistic Re - 幕布</title>
<meta charset="utf-8"/>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="renderer" content="webkit"/>
<meta name="author" content="mubu.com"/>
</head>
<body style="margin: 50px 20px;color: #333;font-family: 'Helvetica Neue','Hiragino Sans GB','WenQuanYi Micro Hei','Microsoft Yahei',sans-serif;">
<div class="export-wrapper"><div style="font-size: 24px; padding: 20px 15px 0;"><div style="padding-bottom: 10px">2.3 逻辑回归的代价函数（Logistic Regression Cost Function）</div><div style="background: #e5e6e8; height: 1px; margin-bottom: 20px;"></div></div><ul style="list-style: disc outside;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">为什么需要代价函数：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">通过训练代价函数来得到逻辑回归模型的参数$w$和参数$b$。</span></li><li style="line-height: 22px;"><span class="content mubu-node" images="%5B%7B%22id%22%3A%222cf16335970423036-733882%22%2C%22uri%22%3A%22document_image%2F0ff1d99c-16b5-4eef-a606-cd629861db9f-733882.jpg%22%2C%22ow%22%3A600%2C%22oh%22%3A133%7D%5D" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;"></span><div style="padding: 3px 0"><img src="https://img.mubu.com/document_image/0ff1d99c-16b5-4eef-a606-cd629861db9f-733882.jpg" style="max-width: 720px;" class="attach-img"></div></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">为了让模型通过学习调整参数，你需要给予一个$m$样本的训练集，这会让你在训练集上找到参数$w$和参数$b$,，来得到你的输出。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">训练集的预测值为y^，我们更希望它会接近于训练集中的y^(i)值，上面的定义是对一个训练样本来说的，这种形式适用于每个训练样本，我们使用这些带有圆括号的上标来区分索引和样本，训练样本$i$所对应的预测值是y^(i),是用训练样本的w^T*w^(i)+b然后通过sigmoid函数来得到，也可以把$z$定义为z^(i)=w^T*x^(i)+b,我们将使用这个符号$(i)$注解，上标$(i)$来指明数据表示$x$或者$y$或者$z$或者其他数据的第$i$个训练样本。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">损失函数：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function:<span class="bold" style="font-weight: bold;">δ（y^,y）=1/2*(y^-y)**2</span></span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我们通过这个δ来衡量预测输出值和实际值有多接近。一般我们用<span class="bold" style="font-weight: bold;">预测值和实际值的平方差或者它们平方差的一半，</span>但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">我们在逻辑回归中用到的损失函数是：δ（y^,y）=-(y㏒y^ + (1-y)㏒(1-y^))</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">为什么要用这个函数作为逻辑损失函数？当我们使用平方误差作为损失函数的时候，你会想要让这个误差尽可能地小，对于这个逻辑回归损失函数，我们也想让它尽可能地小，为了更好地理解这个损失函数怎么起作用，我们举两个例子：</span><ul class="children" style="list-style: disc outside; padding-bottom: 4px;"><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">当$y=1$时损失函数δ（y^,y）=- ㏒y^，如果想要损失函数δ（y^,y）尽可能得小，那么就要y^尽可能大，因为sigmoid函数取值$[0,1]$，所以y^会无限接近于1。</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">当$y=0$时损失函数δ（y^,y)=-㏒(1-y^)，如果想要损失函数δ（y^,y)尽可能得小，那么y^就要尽可能小，因为sigmoid函数取值$[0,1]$，所以y^会无限接近于0。</span></li></ul></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">在这门课中有很多的函数效果和现在这个类似，就是如果$y$等于1，我们就尽可能让$\hat{y}$变大，如果$y$等于0，我们就尽可能让 $\hat{y}$ 变小。 损失函数是在单个训练样本中定义的，它衡量的是算法在单个训练样本中表现如何，</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">cost function</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">为了衡量算法在全部训练样本上的表现如何，我们需要定义一个算法的代价函数，算法的代价函数是:</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">J(w,b) = 1/m(m∑i=1)δ(y^(i),y(i)) = -1/m(m∑i=1)[y(i)㏒y^(i) + (1-y(i)㏒(1-Y^(i)))]</span></li><li style="line-height: 22px;"><span class="content mubu-node" style="line-height: 22px; min-height: 22px; font-size: 14px; display: inline-block; vertical-align: top;">损失函数只适用于像这样的单个训练样本，而代价函数是参数的总代价，所以在训练逻辑回归模型时候，我们需要找到合适的$w$和$b$，来让代价函数  $J$ 的总代价降到最低。 根据我们对逻辑回归算法的推导及对单个样本的损失函数的推导和针对算法所选用参数的总代价函数的推导，结果表明逻辑回归可以看做是一个非常小的神经网络。</span></li></ul></li></ul></div>

</body>
</html>